{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac2e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/Profluent-AI/E1.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea1c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cuda_capabilities = torch.cuda.get_device_capability(0)\n",
    "    if cuda_capabilities[0] >= 8:\n",
    "        print(\"CUDA 8.0 or higher detected; installing flash-attention\")\n",
    "        !pip install flash-attn --no-build-isolation\n",
    "    else:\n",
    "        print(\"CUDA capability lower than 8.0; will not be using flash attention\")\n",
    "else:\n",
    "    print(\"CUDA not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f61dd9",
   "metadata": {},
   "source": [
    "### Scoring substitution variants of a protein\n",
    "\n",
    "In this notebook, we will use a DMS Assay (ID: AMIE_PSEAE_Wrenbeck_2017) from Protein Gym (https://proteingym.org/) containing substitution variants of a protein (Uniprot entry https://www.uniprot.org/uniprotkb/P11436/entry) to show zero-shot fitness prediction. We willcompute score of each variant using masked marginal method using the `E1` model directly in both single sequence and retrieval augmented mode and measure correlation with experimental fitness values. For an explanation of the masked marginal method, please refer to this [paper](https://proceedings.neurips.cc/paper/2021/file/f51338d736f95dd42427296047067694-Supplemental.pdf). In short, we replace each mutated position with mask token and compute the log probability of the actual residue at that position. We then compute the score of the single substitution mutant as the difference in log probability between the mutant and the wildtype. The score for multiple substitutions is computed as the sum of the scores of the individual single substitutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba249320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from E1.batch_preparer import E1BatchPreparer\n",
    "from E1.modeling import E1ForMaskedLM\n",
    "\n",
    "device = torch.device(\"cuda\", torch.cuda.current_device()) if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model = E1ForMaskedLM.from_pretrained(\"Profluent-Bio/E1-300m\").to(device).eval()\n",
    "batch_preparer = E1BatchPreparer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f00a14",
   "metadata": {},
   "source": [
    "### Working in Single Sequence Mode\n",
    "\n",
    "In this section, we will use the model in single sequence mode (i.e we will not pass any homolog sequences as context) to compute the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7c659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "wildtype_sequence = (\n",
    "    \"MRHGDISSSNDTVGVAVVNYKMPRLHTAAEVLDNARKIAEMIVGMKQGLPGMDLVVFPEYSLQGIMYDPAEMMETAVAIPGEETE\"\n",
    "    \"IFSRACRKANVWGVFSLTGERHEEHPRKAPYNTLVLIDNNGEIVQKYRKIIPWCPIEGWYPGGQTYVSEGPKGMKISLIICDDGNY\"\n",
    "    \"PEIWRDCAMKGAELIVRCQGYMYPAKDQQVMMAKAMAWANNCYVAVANAAGFDGVYSYFGHSAIIGFDGRTLGECGEEEMGIQYAQL\"\n",
    "    \"SLSQIRDARANDQSQNHLFKILHRGYSGLQASGDGDRGLAECPFEFYRTWVTDAEKARENVERLTRSTTGVAQCPVGRLPYEGLEKEA\"\n",
    ")\n",
    "\n",
    "# Read the DMS assay data\n",
    "mutated_sequences = pl.read_csv(\"https://huggingface.co/datasets/Profluent-Bio/AMIE_PSEAE_Wrenbeck_2017_example/resolve/main/AMIE_PSEAE_Wrenbeck_2017.csv\")\n",
    "print(mutated_sequences.head())\n",
    "\n",
    "# Figure out the positions where substitutions are made in the wildtype sequence (0-indexed)\n",
    "mutated_positions = sorted(set([int(x[1:-1]) - 1 for x in mutated_sequences[\"mutant\"]]))\n",
    "len(mutated_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabedf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each mutated position, create a masked sequence by replacing the wildtype residue with mask token\n",
    "masked_sequences = [wildtype_sequence[:mp] + \"?\" + wildtype_sequence[mp + 1 :] for mp in mutated_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9c44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we show an example of how the input sequences are converted to relevant information for the model's forward pass.\n",
    "batch = batch_preparer.get_batch_kwargs(masked_sequences[:4], device=device)\n",
    "input_ids = batch[\"input_ids\"]\n",
    "\n",
    "for i in range(min(4, input_ids.shape[0])):\n",
    "    print(batch_preparer.tokenizer.decode(input_ids[i].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49402d",
   "metadata": {},
   "source": [
    "Since we have large number of masked sequences, we will process them in batches of 4 so as not to overwhelm the GPU memory.\n",
    "For each sequence, we will extract the log-probabilities of all the residues of the masked sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d76d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "log_probs_for_all = []\n",
    "\n",
    "for batch_idx in tqdm(range(0, len(masked_sequences), 4)):\n",
    "    batch = batch_preparer.get_batch_kwargs(masked_sequences[batch_idx : batch_idx + 4], device=device)\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device.type, dtype=torch.bfloat16, enabled=device.type == \"cuda\"):\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                within_seq_position_ids=batch[\"within_seq_position_ids\"],\n",
    "                global_position_ids=batch[\"global_position_ids\"],\n",
    "                sequence_ids=batch[\"sequence_ids\"],\n",
    "                past_key_values=None,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "            )\n",
    "\n",
    "    logits: torch.Tensor = outputs.logits  # (B, L, V)\n",
    "    embeddings: torch.Tensor = outputs.embeddings  # (B, L, E)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Boolean Selectors of shape (B, L) to get relevant tokens from logits/embeddings\n",
    "    # last_sequence_selector: True for tokens that are part of the last sequence (including boundary tokens) in case of multi-sequence input.\n",
    "    last_sequence_selector = batch[\"sequence_ids\"] == batch[\"sequence_ids\"].max(dim=1)[0][:, None]\n",
    "    # residue_selector: True for tokens that are part of the input sequence i.e not boundary tokens like 1, 2, <bos>, <eos>, <pad>, etc.\n",
    "    residue_selector = ~(batch_preparer.get_boundary_token_mask(batch[\"input_ids\"]))\n",
    "    # last_sequence_residue_selector: True for residues that are part of the last sequence (excluding boundary tokens)\n",
    "    last_sequence_residue_selector = last_sequence_selector & residue_selector\n",
    "\n",
    "    log_probs = [log_probs[i, last_sequence_residue_selector[i]].cpu().numpy()[None, :] for i in range(input_ids.shape[0])]\n",
    "    log_probs_for_all.extend(log_probs)\n",
    "\n",
    "log_probs_for_all = np.concatenate(log_probs_for_all, axis=0)\n",
    "log_probs_for_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35046370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from tqdm import tqdm\n",
    "\n",
    "vocab = batch_preparer.tokenizer.get_vocab()\n",
    "\n",
    "predicted_scores, fitness_scores = [], []\n",
    "for row in mutated_sequences.iter_rows(named=True):\n",
    "    mutated_position, wildtype_aa, mutant_aa = int(row[\"mutant\"][1:-1]) - 1, row[\"mutant\"][0], row[\"mutant\"][-1]\n",
    "    position_log_probs = log_probs_for_all[mutated_positions.index(mutated_position)]\n",
    "    mutant_score = (position_log_probs[mutated_position, vocab[mutant_aa]] - position_log_probs[mutated_position, vocab[wildtype_aa]]).item()\n",
    "    predicted_scores.append(mutant_score)\n",
    "    fitness_scores.append(row[\"DMS_score\"])\n",
    "\n",
    "print(spearmanr(predicted_scores, fitness_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477aa67b",
   "metadata": {},
   "source": [
    "### Working in Retrieval Augmented Mode\n",
    "\n",
    "In this section, we will an MSA to sample homolog sequences for the wildtype sequence used above and pass them to the model as part of the context. The context sequence returned by sample context function is simply a string of homolog protein sequences separated by commas. It is concatenated with each of our masked sequences from above and passed to the model as part of the input.\n",
    "\n",
    "We use PoET style strategy to sample the homologs from the MSA. Homologs are sampled with weights inversely proportional to the number of their neighbors (sequences in the MSA that are at least 80% identical to them) and are additionally constrained to satisfy a specified maximum similarity to the wildtype sequence.\n",
    "\n",
    "But it is not necessary to use PoET style strategy. You can use any other strategy to sample the homologs (for example, experimentally derived high fitness homologs).\n",
    "\n",
    "Note: You will very likely encounter CUDA OOM error if using T4 gpu on colab. We recommend using A100 or L40 gpu when working in retrieval augmented mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f70697",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://huggingface.co/datasets/Profluent-Bio/AMIE_PSEAE_Wrenbeck_2017_example/resolve/main/msa.a3m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6151561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from E1.msa_sampling import sample_context\n",
    "\n",
    "context, _ = sample_context(\n",
    "    msa_path=\"msa.a3m\",\n",
    "    # Maximum number of sequences that can be in context (hard limit of 511)\n",
    "    max_num_samples=511,\n",
    "    # Total number of residues in the context\n",
    "    max_token_length=14784,\n",
    "    # Maximum similarity of any context sequence to the query sequence\n",
    "    max_query_similarity=0.95,\n",
    "    # Minimum similarity of any context sequence to the query sequence\n",
    "    min_query_similarity=0.0,\n",
    "    # Minimum similarity between any two context sequences for them to be considered neighbors\n",
    "    neighbor_similarity_lower_bound=0.8,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f75b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the context sequence to each of the masked sequence we used previously to compute the scores using\n",
    "# masked marginal method.\n",
    "\n",
    "masked_sequences = [context + \",\" + wildtype_sequence[:mp] + \"?\" + wildtype_sequence[mp + 1 :] for mp in mutated_positions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b29eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we show an example of how the input sequences are converted to relevant information for the model's forward pass.\n",
    "batch = batch_preparer.get_batch_kwargs(masked_sequences[:4], device=device)\n",
    "input_ids = batch[\"input_ids\"]\n",
    "\n",
    "for i in range(min(1, input_ids.shape[0])):\n",
    "    print(batch_preparer.tokenizer.decode(input_ids[i].tolist(), skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc9bde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "log_probs_for_all = []\n",
    "\n",
    "for batch_idx in tqdm(range(0, len(masked_sequences), 4)):\n",
    "    batch = batch_preparer.get_batch_kwargs(masked_sequences[batch_idx : batch_idx + 4], device=device)\n",
    "    input_ids = batch[\"input_ids\"]\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device.type, dtype=torch.bfloat16, enabled=device.type == \"cuda\"):\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                within_seq_position_ids=batch[\"within_seq_position_ids\"],\n",
    "                global_position_ids=batch[\"global_position_ids\"],\n",
    "                sequence_ids=batch[\"sequence_ids\"],\n",
    "                past_key_values=None,\n",
    "                use_cache=False,\n",
    "                output_attentions=False,\n",
    "                output_hidden_states=False,\n",
    "            )\n",
    "\n",
    "    logits: torch.Tensor = outputs.logits  # (B, L, V)\n",
    "    embeddings: torch.Tensor = outputs.embeddings  # (B, L, E)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "\n",
    "    # Boolean Selectors of shape (B, L) to get relevant tokens from logits/embeddings\n",
    "    # last_sequence_selector: True for tokens that are part of the last sequence (including boundary tokens) in case of multi-sequence input.\n",
    "    last_sequence_selector = batch[\"sequence_ids\"] == batch[\"sequence_ids\"].max(dim=1)[0][:, None]\n",
    "    # residue_selector: True for tokens that are part of the input sequence i.e not boundary tokens like 1, 2, <bos>, <eos>, <pad>, etc.\n",
    "    residue_selector = ~(batch_preparer.get_boundary_token_mask(batch[\"input_ids\"]))\n",
    "    # last_sequence_residue_selector: True for residues that are part of the last sequence (excluding boundary tokens)\n",
    "    last_sequence_residue_selector = last_sequence_selector & residue_selector\n",
    "\n",
    "    log_probs = [log_probs[i, last_sequence_residue_selector[i]].cpu().numpy()[None, :] for i in range(input_ids.shape[0])]\n",
    "    log_probs_for_all.extend(log_probs)\n",
    "\n",
    "log_probs_for_all = np.concatenate(log_probs_for_all, axis=0)\n",
    "log_probs_for_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ffef99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "vocab = batch_preparer.tokenizer.get_vocab()\n",
    "\n",
    "predicted_scores, fitness_scores = [], []\n",
    "for row in mutated_sequences.iter_rows(named=True):\n",
    "    mutated_position, wildtype_aa, mutant_aa = int(row[\"mutant\"][1:-1]) - 1, row[\"mutant\"][0], row[\"mutant\"][-1]\n",
    "    position_log_probs = log_probs_for_all[mutated_positions.index(mutated_position)]\n",
    "    mutant_score = (position_log_probs[mutated_position, vocab[mutant_aa]] - position_log_probs[mutated_position, vocab[wildtype_aa]]).item()\n",
    "    predicted_scores.append(mutant_score)\n",
    "    fitness_scores.append(row[\"DMS_score\"])\n",
    "\n",
    "print(spearmanr(predicted_scores, fitness_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88be625f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
